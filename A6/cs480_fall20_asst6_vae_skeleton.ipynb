{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cs480_fall20_asst6_vae_skeleton.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29l1O26v4Gti"
      },
      "source": [
        "# Generalities for DAEs and VAEs\n",
        "\n",
        "* Import libraries\n",
        "* Run on GPU if possible\n",
        "* Create data loaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGQp6JTfPKUL",
        "outputId": "976c8713-4915-4e39-9caf-5a15526fdeed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import torch\n",
        "import torch.utils.data\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image\n",
        "from IPython.display import Image, display\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# create directory to save results\n",
        "!mkdir results\n",
        "\n",
        "# script parameters\n",
        "batch_size = 128\n",
        "log_interval = 100\n",
        "\n",
        "# run on GPU if possible\n",
        "cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
        "\n",
        "# create data loaders\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                   transform=transforms.ToTensor()),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.ToTensor()),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory 鈥榬esults鈥�: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgYN7IQp408y"
      },
      "source": [
        "# Neural Network classes\n",
        "\n",
        "* Deterministic auto-encoder (DAE)\n",
        "* Variational auto-encoder (VAE)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLmnOEmr36YD"
      },
      "source": [
        "# Deterministic Auto-Encoder (DAE)\n",
        "class DAE(nn.Module):\n",
        "\n",
        "    # declare layers\n",
        "    def __init__(self):\n",
        "        super(DAE, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 400)\n",
        "        self.fc2 = nn.Linear(400, 20)\n",
        "        self.fc3 = nn.Linear(20, 400)\n",
        "        self.fc4 = nn.Linear(400, 784)\n",
        "\n",
        "    # encoder: one ReLU hidden layer of 400 nodes, one Linear output layer of 20 nodes\n",
        "    def encode(self, x):\n",
        "        h = F.relu(self.fc1(x))\n",
        "        return self.fc2(h)\n",
        "\n",
        "    # decoder: one ReLU hidden layer of 400 nodes, one sigmoid output layer of 784 nodes\n",
        "    def decode(self, z):\n",
        "        h3 = F.relu(self.fc3(z))\n",
        "        return torch.sigmoid(self.fc4(h3))\n",
        "\n",
        "    # forward: encoder followed by decoder\n",
        "    def forward(self, x):\n",
        "        z = self.encode(x.view(-1, 784))\n",
        "        return self.decode(z)\n",
        "\n",
        "# Variational Auto-Encoder (VAE)     \n",
        "class VAE(nn.Module):\n",
        "\n",
        "    # declare layers\n",
        "    def __init__(self):\n",
        "        super(VAE, self).__init__()\n",
        "\n",
        "        # dummy assignment until this function is filled in\n",
        "        self.fc1 = nn.Linear(1, 1)\n",
        "\n",
        "\n",
        "    # Encoder: one ReLU hidden layer of 400 nodes, two Linear output layers of 20 nodes (means and log_variances)\n",
        "    # Input: image (array of size 784)\n",
        "    # Output: means (array of size 20), log_variances (array of size 20)\n",
        "    def encode(self, x):\n",
        "\n",
        "        # dummy assignment until this function is filled in\n",
        "        means = torch.randn(x.shape[0], 20).to(device)\n",
        "        log_variances = torch.randn(x.shape[0], 20).to(device)\n",
        "        return means, log_variances\n",
        "\n",
        "    # Reparameterization:\n",
        "    #\n",
        "    # Let eps be samples from a Normal(0,1)\n",
        "    # Input: means (array of size 20), log_variances (array of size 20)\n",
        "    # Output: embedding (array of size 20 corresponding to means + eps * exponential(log_variances/2))\n",
        "    def reparameterize(self, means, log_variances):\n",
        "\n",
        "        # dummy assignment until this function is filled in\n",
        "        embedding = torch.randn(means.shape[0], 20).to(device)\n",
        "        return embedding\n",
        "\n",
        "    # Decoder: one ReLU hidden layer of 400 nodes, one sigmoid output layer of 784 nodes\n",
        "    # Input: embedding (array of size 20)\n",
        "    # Output: probability of reconstructed image (array of size 784)\n",
        "    def decode(self, z):\n",
        "\n",
        "        # dummy assignment until this function is filled in\n",
        "        probabilties = torch.rand(z.shape[0],784).to(device)\n",
        "        return probabilties\n",
        "\n",
        "    # Function forward: encoder, reparameterize, decoder\n",
        "    # Input: image\n",
        "    # Output: probability of reconstructed image, means, log_variances\n",
        "    def forward(self, x):\n",
        "\n",
        "        # dummy assignment until this function is filled in\n",
        "        probabilties = torch.rand(x.shape[0],20).to(device)\n",
        "        means = torch.randn(x.shape[0], 20).to(device)\n",
        "        log_variances = torch.randn(x.shape[0], 20).to(device)\n",
        "        return probabilties, means, log_variances"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptSvrR4255tn"
      },
      "source": [
        "# Loss Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1AjZK3p4XB8"
      },
      "source": [
        "# DAE Loss Function\n",
        "# Reconstruction loss: binary cross entropy\n",
        "#\n",
        "# Inputs:\n",
        "#    reconstructed_x: reconstructed image\n",
        "#    x: image\n",
        "#\n",
        "# Output:\n",
        "#    BCE: binary cross entropy  \n",
        "#\n",
        "def dae_loss_function(reconstructed_x, x):\n",
        "    BCE = F.binary_cross_entropy(reconstructed_x, x.view(-1, 784), reduction='sum')\n",
        "    return BCE\n",
        "\n",
        "# VAE Loss Function\n",
        "# Reconstruction loss: binary cross entropy\n",
        "# KL divergence loss: -0.5 * sum(1 + log_variances - means^2 - exp(log_variances))\n",
        "#       see Appendix B from VAE paper:\n",
        "#       Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
        "#       https://arxiv.org/abs/1312.6114\n",
        "#\n",
        "# Inputs: \n",
        "#    reconstructed_x: reconstructed image\n",
        "#    x: image\n",
        "#    means: array of size 20\n",
        "#    log_variances: array of size 20\n",
        "#\n",
        "# Outputs:\n",
        "#    total_loss: binary cross entropy + KL divergence (scalar)\n",
        "#    BCE: binary cross entropy (scalar)\n",
        "#\n",
        "def vae_loss_function(reconstructed_x, x, means, log_variances):\n",
        "\n",
        "    # dummy assignment until this function is filled in\n",
        "    total_loss = 0\n",
        "    BCE = 0\n",
        "    return total_loss, BCE\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAZcoksjDKX8"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M12IvAMcDK59"
      },
      "source": [
        "# DAE Training\n",
        "#\n",
        "# Inputs:\n",
        "#    epoch: epoch #\n",
        "#    model: DAE neural network\n",
        "#    optimizer: DAE optimizer\n",
        "#\n",
        "# Outputs:\n",
        "#    average_train_BCE: binary cross entropy (scalar)\n",
        "#\n",
        "def dae_train(epoch, model, optimizer):\n",
        "    train_BCE = 0\n",
        "    for batch_idx, (data, _) in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        recon_batch = model(data)\n",
        "        loss = dae_loss_function(recon_batch, data)\n",
        "        loss.backward()\n",
        "        train_BCE += loss.item()\n",
        "        optimizer.step()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader),\n",
        "                loss.item() / len(data)))\n",
        "\n",
        "    average_train_BCE = train_BCE / len(train_loader.dataset)\n",
        "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
        "          epoch, average_train_BCE))\n",
        "    return average_train_BCE\n",
        "\n",
        "# VAE Training\n",
        "#\n",
        "# Inputs:\n",
        "#    epoch: epoch #\n",
        "#    model: VAE neural network\n",
        "#    optimizer: VAE optimizer\n",
        "#\n",
        "# Outputs:\n",
        "#    average_train_loss: binary cross entropy + KL divergence (scalar)\n",
        "#    average_train_BCE: binary cross entropy (scalar)\n",
        "#\n",
        "def vae_train(epoch, model, optimizer):\n",
        "\n",
        "    # dummy assignment until this function is filled in\n",
        "    average_train_loss = 0\n",
        "    average_train_BCE = 0\n",
        "    return average_train_loss, average_train_BCE\n",
        "\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVyi8tVUE2CF"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvHlg5KqE2ZF"
      },
      "source": [
        "# DAE Test\n",
        "#\n",
        "# Inputs:\n",
        "#    epoch: epoch #\n",
        "#    model: DAE neural network\n",
        "#\n",
        "# Outputs:\n",
        "#    average_test_BCE: binary cross entropy (scalar)\n",
        "#\n",
        "def dae_test(epoch, model):\n",
        "    model.eval()\n",
        "    test_BCE = 0\n",
        "    with torch.no_grad():\n",
        "        for i, (data, _) in enumerate(test_loader):\n",
        "            data = data.to(device)\n",
        "            recon_batch = model(data)\n",
        "            test_BCE += dae_loss_function(recon_batch, data).item()\n",
        "            if i == 0:\n",
        "                n = min(data.size(0), 8)\n",
        "                comparison = torch.cat([data[:n],\n",
        "                                      recon_batch.view(batch_size, 1, 28, 28)[:n]])\n",
        "                save_image(comparison.cpu(),\n",
        "                         'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
        "                print('Sample of reconstructed images (top row are targets and bottom row are reconstructions)')\n",
        "                display(Image('results/reconstruction_' + str(epoch) + '.png'))\n",
        "\n",
        "    average_test_BCE = test_BCE / len(test_loader.dataset)\n",
        "    print('====> Test set loss: {:.4f}'.format(average_test_BCE))\n",
        "    return average_test_BCE\n",
        "\n",
        "# VAE Test\n",
        "#\n",
        "# Inputs:\n",
        "#    epoch: epoch #\n",
        "#    model: VAE neural network\n",
        "#\n",
        "# Outputs:\n",
        "#    average_test_loss: binary cross entropy + KL divergence (scalar)\n",
        "#    average_test_BCE: binary cross entropy (scalar)\n",
        "#\n",
        "def vae_test(epoch, model):\n",
        "\n",
        "    # dummy assignment until this function is filled in\n",
        "    average_test_loss = 0\n",
        "    average_test_BCE = 0\n",
        "    return average_test_loss, average_test_BCE\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF7wGhW0Ftf0"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQEois6RFvW_"
      },
      "source": [
        "# train and test DAE\n",
        "epochs = 10\n",
        "dae_average_train_BCE = []\n",
        "dae_average_test_BCE = []\n",
        "dae_model = DAE().to(device)\n",
        "dae_optimizer = optim.Adam(dae_model.parameters(), lr=1e-3)\n",
        "for epoch in range(1, epochs + 1):\n",
        "    average_train_BCE = dae_train(epoch, dae_model, dae_optimizer)\n",
        "    dae_average_train_BCE.append(average_train_BCE)\n",
        "    average_test_BCE = dae_test(epoch, dae_model)\n",
        "    dae_average_test_BCE.append(average_test_BCE)\n",
        "    with torch.no_grad():\n",
        "        sample = torch.randn(64, 20).to(device)\n",
        "        sample = dae_model.decode(sample).cpu()\n",
        "        save_image(sample.view(64, 1, 28, 28),\n",
        "                   'results/sample_' + str(epoch) + '.png')\n",
        "        print('Sample of generated images')\n",
        "        display(Image('results/sample_' + str(epoch) + '.png'))\n",
        "        print('\\n')\n",
        "\n",
        "# train and test VAE\n",
        "epochs = 10\n",
        "vae_average_train_losses = []\n",
        "vae_average_train_BCE = []\n",
        "vae_average_test_losses = []\n",
        "vae_average_test_BCE = []\n",
        "vae_model = VAE().to(device)\n",
        "vae_optimizer = optim.Adam(vae_model.parameters(), lr=1e-3)\n",
        "for epoch in range(1, epochs + 1):\n",
        "    average_train_loss, average_train_BCE = vae_train(epoch, vae_model, vae_optimizer)\n",
        "    vae_average_train_losses.append(average_train_loss)\n",
        "    vae_average_train_BCE.append(average_train_BCE)\n",
        "    average_test_loss, average_test_BCE = vae_test(epoch, vae_model)\n",
        "    vae_average_test_losses.append(average_test_loss)\n",
        "    vae_average_test_BCE.append(average_test_BCE)\n",
        "    with torch.no_grad():\n",
        "        sample = torch.randn(64, 20).to(device)\n",
        "        sample = vae_model.decode(sample).cpu()\n",
        "        save_image(sample.view(64, 1, 28, 28),\n",
        "                   'results/sample_' + str(epoch) + '.png')\n",
        "        print('Sample of generated images')\n",
        "        display(Image('results/sample_' + str(epoch) + '.png'))\n",
        "        print('\\n')\n",
        "\n",
        "# Plot Train BCE Losses\n",
        "plt.plot(dae_average_train_BCE)\n",
        "plt.plot(vae_average_train_BCE)\n",
        "plt.title('Train BCE Losses')\n",
        "plt.ylabel('Binary Cross Entropy')\n",
        "plt.xlabel('Epoch #')\n",
        "plt.legend(['DAE','VAE'], loc='upper right')\n",
        "plt.show()\n",
        "\n",
        "# Plot Test BCE Losses\n",
        "plt.plot(dae_average_test_BCE)\n",
        "plt.plot(vae_average_test_BCE)\n",
        "plt.title('Test BCE Losses')\n",
        "plt.ylabel('Binary Cross Entropy')\n",
        "plt.xlabel('Epoch #')\n",
        "plt.legend(['DAE','VAE'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}